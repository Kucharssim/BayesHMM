---
title: "An introduction to BayesHMM"
author: "Luis Damiano, Michael Weylandt, Brian Peterson"
date: "`r Sys.Date()`"
output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{An introduction to BayesHMM}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
urlcolor: blue
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo     = TRUE, 
  eval     = TRUE, 
  cache    = TRUE,
  collapse = TRUE,
  comment  = "#>"
)
```

% Math operators
\newcommand{\argmax}{\arg\!\max}
\newcommand{\argmin}{\arg\!\min}
\newcommand\ev[1]{E\left\langle#1\right\rangle}
\newcommand\vv[1]{V\left\langle#1\right\rangle}

% Math commands
\newcommand{\mat}[1]{\mathbf{#1}}

% Math symbols
\newcommand{\DD}{\mathcal{D}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\UU}{\mathcal{U}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\RR}{\mathbb{R}}

BayesHMM is an R Package to run full Bayesian inference on Hidden Markov Models (HMM) using the probabilistic programming language Stan. By providing an intuitive, expressive yet flexible input interface, we enable researchers to profit the most out of the Bayesian workflow. We provide the user with an expressive interface to mix and match a wide array of options for the observation and latent models, including ample choices of densities, priors, and link functions whenever covariates are present. The software enables users to fit HMM with time-homogeneous transitions as well as time-varying transition probabilities. Priors can be set for every model parameter. Implemented inference algorithms include forward (filtering), forward-backwards (smoothing), Viterbi (most likely hidden path), prior predictive sampling, and posterior predictive sampling. Graphs, tables and other convenience methods for convergence diagnosis, goodness of fit, and data analysis are provided.

This vignette introduces the software and briefly reviews current capabilities. Although we walk the reader through most of the functionalities, we do not discuss function arguments, implementation details, and other details typically reviewed in the documentations. A list of future features may be found in the [Roadmap wiki page](https://github.com/luisdamiano/BayesHMM/wiki/Roadmap).

We acknowledge [Google Summer of Code 2018](https://summerofcode.withgoogle.com/projects/#4681157036212224) for funding.

__NOTE__: Our software is work in progress. We will review naming conventions as well as other design decisions at a later stage. Fully detailed documentation will be available at the time of release.

# Introduction

# Hidden Markov Models

Hidden Markov Models (HMM) involve two interconnected models. The state model consists of a discrete-time, discrete-state first-order Markov chain $z_t \in \{1, \dots, K\}$ that transitions according to $p(z_t | z_{t-1})$. In turns, the observation model is governed by $p(\mat{y}_t | z_t)$, where $\mat{y}_t \in \RR^R$ are the observations, emissions or output. The corresponding joint distribution is

\[
p(\mat{z}_{1:T}, \mat{y}_{1:T})
  = p(\mat{z}_{1:T}) p(\mat{y}_{1:T} | \mat{z}_{1:T})
  = \left[ p(z_1) \prod_{t=2}^{T}{p(z_t | z_{t-1})} \right] \left[ \prod_{t=1}^{T}{p(\mat{y}_t | z_{t})} \right].
\]

The non-stochastic quantities of the model are the length of the observed sequence $T$ and the number of hidden states $K$. The observed sequence $\mat{y}_t$ is a stochastic known quantity. The parameters of the models are $\mat{\theta} = (\mat{\pi}, \mat{\theta}_t, \mat{\theta}_o)$, where $\mat{\pi}$ are the probabilities of the initial state distribution, $\mat{\theta}_t$ are the parameters of the transition model and $\mat{\theta}_o$ are the parameters of the state-conditional density function $p(\mat{y}_t | z_t)$. Their forms depend on the characteristics of the model specified by the user.

## Observation model

The observation model is specified by the density $p(\mat{y}_t | z_{t})$. When the output is discrete, it commonly takes the form of an observation matrix

\[
p(\mat{y}_t | z_t = k, \mat{\theta}) = \text{Categorical}(\mat{y}_t | \mat{\theta}_k).
\]

If the output is continuous, observations may follow for example a conditional Gaussian distribution

\[
p(\mat{y}_t | z_t = k, \mat{\theta}) = \mathcal{N}(\mat{y}_t | \mat{\mu}_k, \mat{\Sigma}_k).
\]

Alternatively, time-varying covariates $\mat{x}_t \in \RR^M$ may be used to drive the location parameter of the chosen density,

\[
p(\mat{y}_t | \mat{x}_t, z_t = k, \mat{\theta}) = \mathcal{N}(\mat{y}_t | \mat{x}_t \mat{\beta}^x_k, \mat{\Sigma}_k).
\]

## Transition model

In the most common case of time-homogeneous HMMs, state transitions are characterized by the $K \times K$ sized transition matrix with simplex rows $\mat{A} = \{a_{ij}\}$ with $a_{ij} = p(z_t = j | z_{t-1} = i)$. There are $K \times (K-1)$ free parameters as the rows of the matrix must sum to one.

Alternatively, time-varying covariates $\mat{u}_t \in \RR^P$ may be used to drive the location parameter of the chosen density, effectively allowing for time-varying transition probabilities. The model involves a multinomial regression whose parameters depend on the previous state taking the value $i$. Using the softmax transform, for example,

\[
p(z_t | \mat{u}_{t}, z_{t-1} = i) = \text{softmax}(\mat{u}_{t} \mat{\beta}^u_i).
\]

## Initial distribution model

Most frequently in applications, the initial distribution model is characterized by a $K$ sized simplex $\mat{\pi}$ with initial probabilities $\pi_{i} = p(z_1 = i)$. Alternatively, the first observation may be assigned to a state via a multinomial regression if relevant information is available in the form of a covariate vector $\mat{v} \in \RR^Q$,

\[
p(z_1 | \mat{v}, \mat{\theta}) = \text{softmax}(\mat{v} \mat{\beta}^v).
\]

## Naming convention

Documentation and software adopts the following naming convention. Minor modifications may be found due to syntax restrictions in R or Stan.

```
Constants
R		    Observation dimension
K		    Number of hidden states
M       Number of covariates for the observation model
P       Number of covariates for the transition model
Q       Number of covariates for the initial model

Covariates
x_t		  Time-varying covariates for the observation model
u_t		  Time-varying covariates for the transition model
v  		  Covariates for the initial model

Known-stochastic quantities
y_t		  Observation vector

Model parameters
*_kr		Ex. mu_11, sigma_11 (k, r suffixes are optional)
A		    Transition model parameters (if no covariates)
pi		  Initial distribution parameters (if no covariates)
xBeta	  Regression parameters for the observation model
uBeta	  Regression parameters for the transition model
vBeta	  Regression parameters for the initial model

Estimated quantities
z_t 	  Hidden state
alpha_t	Filtered probability
gamma_t	Smoothed probability
zstar		Viterbi

(Prior/Posterior) predictive quantities
yPred		Sample of observations drawn from the  predictive density
zPred		Sample of latent path drawn from the  predictive density

Note: time suffix t is optional.
```

# Using BayesHMM

The typical data analysis workflow includes the following steps: (1) specify, (2) validate, (3) simulate, (4) fit, (5) diagnose, (6) visualize, (7) compare. All steps except for 5 and 7 are implemented as of today.

You may install the last version of our software from GitHub. Please note that BayesHMM requires rstan (>= 2.17), the R interface for the probabilistic programming language [Stan](http://mc-stan.org/).

```{r, echo = FALSE}
library(BayesHMM)
```

```{r eval = FALSE}
devtools::install_github("luisdamiano/BayesHMM", ref = "master")
library(BayesHMM)
```

## 1. Specify

A HMM may be specified by calling the `hmm` function:

```{r, eval = FALSE}
hmm(
  K = 3, R = 2,
  observation = {...}
  initial     = {...}
  transition  = {...}
  name = "Model name"
)
```

where $K$ is the number of hidden states and $R$ is the number of dimensions in the observation variable.

The `observation`, `initial`, and `transition` fields rely on S3 objects called `Density` to specify density form, parameter priors, and fixed values for parameters. User may specify bounds in the parameter space as well as truncation in prior densities. For example:

* `Gaussian(mu = 0, sigma = 1)` specifies a Gaussian density with fixed parameters.
* `Gaussian(mu = Gaussian(0, 10), sigma = Cauchy(0, 10, bounds = list(0, NA)))` specifies a Gaussian density with a Gaussian prior on the location parameter and a Cauchy prior on the scale parameter, which is bounded on $[0, \infty)$.

Currently available densities are listed below:

* Observation model:
    * Univariate: Bernoulli, Beta, Binomial, Categorical, Cauchy, Dirichlet, Gaussian, Multinomial, Negative Binomial (traditional and location parameterizations), Poisson, Student.
    * Multivariate: Multivariate Gaussian (traditional, Cholesky decomposition of covariance matrix, and Cholesky decomposition of correlation matrix parameterizations), Multivariate Student.
    * Regressions: Bernoulli regression with logit link, Binomial regression (logit and probit links), Softmax regression, Gaussian regression.
    * Prior-only density: LKJ, Wishart.
* Transition model: Dirichlet, Softmax regression.
* Initial model: Dirichlet, Softmax regression.

### Observation model

Internally, a specification stores either $K$ multivariate densities (i.e. one multivariate density for state) or $K \times R$ univariate densities (i.e. one univariate density for each dimension in the observation variable and each state). As shown in the next table, densities are recycled to simplify user-input code.

  |R    | User input | Density             | Action      |
  |-----|------------|---------------------|-------------|
  | 1   | 1          | UnivariateDensity   | repeat_K    |
  | 1   | K          | UnivariateDensity   | repeat_none |
  |>1   | 1          | MultivariateDensity | repeat_K    |
  |>1   | 1          | UnivariateDensity   | repeat_KxR  |
  |>1   | K          | MultivariateDensity | repeat_none |
  |>1   | K          | UnivariateDensity   | repeat_R    |
  |>1   | R          | UnivariateDensity   | repeat_K    |
  |>1   | KxR        | UnivariateDensity   | repeat_none |

For a univariate model ($R = 1$):

  * User enters one univariate density: specify the same density in each hidden state variable.
  * User enters $K$ univariate density: specify one different density in each hidden state (ex. Gaussian in one state and Student in the other).

This system is flexible enough to let the user specify different densities and priors for each state. The following snippet specify a (rather unrealistic) model where states are a priori believed to have negative, near zero, and positive location parameters.
      
```{r}
mySpec <- hmm(
  K = 3, R = 1,
  observation =
    Gaussian(mu = -10, sigma = 10) +
    Gaussian(mu =   0, sigma = 10) +
    Gaussian(mu =  10, sigma = 10),
  initial     = Dirichlet(alpha = Default()),
  transition  = Dirichlet(alpha = Default()),
  name = "Univariate Gaussian"
)
```

For a multivariate model ($R > 1$):

  * User enters one univariate density: same density for each dimension of the observation variable in all states (i.e. same density and priors for the R variables in the K latent states).
  * User enters one multivariate density: same density for the observation vector in all hidden states.
  * User enters $K$ univariate density: each dimension of the observation variable has the same specification within a each state.
  * User enters $K$ multivariate density: specify a multivariate density for the observation vector for each state.
  * User enters $R$ univariate density: specify one density for each element of the observation vector, which is the same across state.
  * User enters $R \times K$ univariate density: specify density for each element of the observation vector in each state. Order is $k_1 r_1, \dots, k_1, r_R, k_2r_1, \dots, k_2r_R, \dots, k_Kr_1, \dots, k_K,r_R$.
  * When $K = R$, $K$ prevails.

### Transition model

The user may enter a `Density` S3 object such as a `Dirichlet`, or a `LinkDensity` S3 object (inherits from `Density`) such as `TransitionSoftmax()`. The latter allows to specify priors for the transition regression parameter vector. For example, the following snippet sets regularizing priors on the regression coefficients. Note that this model is hard to identify, although we achieved satisfactory results optimizing the posterior density (i.e. maximum a posterior estimates).

```{r}
mySpec <- hmm(
  K = 2, R = 1,
  observation = Gaussian(
    mu    = Gaussian(0, 10),
    sigma = Student(mu = 0, sigma = 10, nu = 1, bounds = list(0, NULL))
  ),
  initial     = Default(),
  transition  = TransitionSoftmax(
    uBeta = Gaussian(0, 5)
  ),
  name = "TVHMM Softmax Univariate Gaussian"
)
```

Internally, a specification stores either $K$ multivariate densities (i.e. one multivariate density for each row in the transition matrix) or $K \times K$ univariate densities (i.e. one univariate density for each element in the transition matrix). Densities are recycled accordingly.

  | User-input | Density             | Action                   |
  |------------|---------------------|--------------------------|
  | 1          | UnivariateDensity   | repeat_KxK               |
  | 1          | MultivariateDensity | repeat_K                 |
  | 1          | LinkDensity         | nothing                  |
  | K          | UnivariateDensity   | repeat_K                 |
  | K          | MultivariateDensity | repeat_none_multivariate |
  | KxK        | UnivariateDensity   | repeat_none_univariate   |

__NOTE__: As of the time of this writing, fixed values for transition parameters have not been implemented yet.

### Initial model

Internally, a specification stores either one multivariate densities for the whole initial probability vector, or $K$ univariate densities for each element of said vector. Densities are recycled accordingly.

  | User-input | Density             | Action
  | -----------|---------------------|--------------------------
  | 1          | UnivariateDensity   | repeat_K
  | 1          | MultivariateDensity | repeat_none_multivariate
  | 1          | LinkDensity         | repeat_none_multivariate
  | K          | UnivariateDensity   | repeat_none_univariate

```{r}
mySpec <- hmm(
  K = 2, R = 1,
  observation = Gaussian(
    mu    = Gaussian(0, 10),
    sigma = Student(mu = 0, sigma = 10, nu = 1, bounds = list(0, NULL))
  ),
  initial     = InitialSoftmax(
    vBeta = Gaussian(0, 5)
  ),
  transition  = Dirichlet(alpha = Default()),
  name = "Univariate Gaussian with covariates for initial model"
)
```

### Examples

Many possible configurations are shown in the Appendix \ref{sec:appendix}.

## 2. Validate

Bayesian Software Validation allows users to test software accuracy [CITE]. We provide a one-liner to run a validation protocol based on the prior predictive density. Iterations are run in parallel using `foreach` and `doParallel`.

__Validation protocol inspired by Simulation Based Calibration (cite)__

* Compile the prior predictive model (i.e. no likelihood statement in the Stan code).
* Draw $N$ samples of the parameter vector $\theta$ and the observation vector $\mat{y}_t$ from prior predictive density.
* Compile the posterior predictive model (i.e. Stan code includes both prior density and likelihood statement).
* For all $n \in 1, \dots, N$:
    * Feed $\mat{y}_t^{(n)}$ to the full model.
    * Draw one posterior sample of the observation variable $\mat{y_t}^{(n)}_{\text{new}}$.
    * Collect Hamiltonian Monte Carlo diagnostics: number of divergences, number of times max tree depth is reached, maximum leapfrogs, warm up and sample times.
    * Collect posterior sampling diagnostics: posterior summary measures (mean, sd, quantiles), comparison against the true value (rank), MCMC convergence measures (Monte Carlo SE, ESS, R Hat).
    * Collect posterior predictive diagnostics: observation ranks, Kolmogorov-Smirnov statistic for observed sample vs posterior predictive samples.
    * Other: we expect to enrich the protocol with more relevant quantities in future iterations.

There is no such a thing as an universal, automatic way to validate models. The software computes and summarizes different diagnostic measures and provides the user with high-level tools to assess calibration (including printouts, tables, and plots). Because the user may want to single out and inspect one of the iterations, in future versions it will be able to recover (or reproduce) the full posterior sample drawn in the $n$-th iteration.

In the following snippet, we specify a HMM and validate its calibration.

```{r, results = "hide"}
  mySpec <- hmm(
    K = 3, R = 2,
    observation =
      Gaussian(mu = Gaussian(mu = -10, sigma = 1), sigma = 1) +
      Gaussian(mu = Gaussian(mu =   0, sigma = 1), sigma = 1) +
      Gaussian(mu = Gaussian(mu =  10, sigma = 1), sigma = 1),
    initial     = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
    transition  =
      Dirichlet(alpha = c(1.0, 0.2, 0.2)) +
      Dirichlet(alpha = c(0.2, 1.0, 0.2)) +
      Dirichlet(alpha = c(0.2, 0.2, 1.0)),
    name = "Dummy Model"
  )

  val <- validate_calibration(mySpec, N = 2, T = 100, iter = 500, seed = 9000)
```

The function returns a named list with two elements, `chains` and `parameters`, which are data frames storing the aforementioned summary quantities.

```{r}
knitr::kable(
  head(val$chains), 
  digits = 2, caption = "Chains diagnostics"
)

knitr::kable(
  head(val$parameters), 
  digits = 2, caption = "Parameters diagnostics"
)

plot(
  val$parameters$Rhat, 
  type = "h", ylim = c(0.9, 1.15),
  ylab = expression(hat(R)),
  xlab = "Validation run (n)"
)
abline(h = 1.1)
```

## 3. Simulate & Fit

Once specified, the user may:

* Call `sim()` to simulate $N$ datasets containing the observation variables $\mat{y}$ and the hidden paths $z$.
* Call `compile()` to compile the underlying Stan model. The returned object may be later used for either sampling (MCMC) or optimization (MAP).
    * Call `sampling()` to draw posterior samples from a compiled model effectively allowing for full Bayesian inference via MCMC.
    * Call `optimizing()` to maximize the joint posterior density and obtain a point estimate.
* Call `fit()` to compile and sample in one step.

After fitting, the underlying Stan code may be inspected by calling `browse_model`, which opens the Stan file in the editor.

### 3.1. Fully Bayesian Inferencia via MCMC

Because `sampling()` and `fit()` return a `stanfit` S4 object, the posterior samples are compatible out of box with every tool in the `rstan` ecosystem [TRY AND LIST SOME SUCH AS bayesplot].

### 3.2. Maximum At Posteriori estimates via posterior optimization

`optimizing` is able to run several initialization of the optimization algorithm in parallel. The user may decide to `keep = "all"` runs, or just `keep = "best"`. If the former, `extract_grid` returns a summary of the different runs and `extract_best` retrieves the one with highest posterior log likelihood. If no run achieves convergence, a warning message is prompt to the console.

```{r, results = "hide"}
mySpec <- hmm(
  K = 3, R = 1,
  observation =
    Gaussian(mu = Gaussian(mu = -10, sigma = 1), sigma = 1) +
    Gaussian(mu = Gaussian(mu =   0, sigma = 1), sigma = 1) +
    Gaussian(mu = Gaussian(mu =  10, sigma = 1), sigma = 1),
  initial     = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  name = "Univariate Gaussian"
)

set.seed(9000)
y <- as.matrix(
  c(rnorm(100, -10, 1), rnorm(100, 0, 1), rnorm(100, 10, 1))
)
y <- sample(y)

myModel <- compile(mySpec)
myAll   <- optimizing(mySpec, myModel, y = y, nRun = 10, keep = "all", nCores = 4)
myBest <- extract_best(myAll)
```

```{r}
print(
  round(extract_grid(myAll, pars = "mu"), 2)
)

plot_obs(myBest, pch = 21, bg = "red")
```

__NOTE__: Passing both the specification and the compiled model is redundant. In future versions, user will not need to pass the specification.

### 3.3 Common high-level routines

Most of these extractors rely on the lower level `extract_quantity` function. By default, it returns a named list where each element is an array representing a sampled quantity. The dimension of the array varies according to the number of iterations, the length of the series, and the structure of the quantity itself (for example, whether they are univariate or multivariate). Additionally, the function accepts wildcards `*` for easy extraction of a family of quantity.

This function has two convenience arguments:

* `reduce` takes a function that is run with-in chain, effectively reducing the $N$ draws per chain into one or more quantity.
* `combine` takes a function that is run on the reduced quantity for each parameter, effectively combining the summary quantity from each parameter into a single data structure.

Besides `extract_quantity`, which takes an argument `pars`, the software comes with extractors for the most typical quantities: `extract_alpha` (filtered probability), `extract_gamma` (smoothed probability), `extract_zstar` (MPM, also known a Viterbi), `extract_ypred` and `extract_zpred` for predictive quantities, and `extract_obs_parameters` and `extract_parameters` for observation model and all model parameters respectively.

__NOTE__: Naming convention may be changed before submitting to CRAN. Consider for example `extract_alpha` versus `extract_filtered_probability`, or `extract_zstar` versus `extract_viterbi`.

Compare in these examples different configurations of `reduce` and `combine` :

```{r}
mySpec <- hmm(
  K = 3, R = 1,
  observation =
    Gaussian(mu = Gaussian(mu = -10, sigma = 1), sigma = 1) +
    Gaussian(mu = Gaussian(mu =   0, sigma = 1), sigma = 1) +
    Gaussian(mu = Gaussian(mu =  10, sigma = 1), sigma = 1),
  initial     = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  name = "Univariate Gaussian"
)

set.seed(9000)
y <- as.matrix(
  c(rnorm(100, -10, 1), rnorm(100, 0, 1), rnorm(100, 10, 1))
)
y <- sample(y)

myFit <- fit(mySpec, y = y)

# 3 parameters, 1,000 iterations, 4 chains
print(
  str(extract_obs_parameters(myFit))
)

# reduce within-chain draws to one quantity (median)
print(
  extract_obs_parameters(myFit, reduce = median)
)

# reduce within-chain draws to two quantities (quantiles)
print(
  extract_obs_parameters(
    myFit, reduce = posterior_intervals(c(0.1, 0.9))
  )
)

# combine quantities into a matrix
print(
  extract_obs_parameters(
      myFit, 
      reduce  = median,
      combine = rbind
  )
)
```

Since classification is a frequent task in the data analysis workflow, we provide a family of `classify_*` functions that extract the estimated quantities and return the most likely state: `classify_alpha` and `classify_gamma` return the state with highest posterior probability at each time step, while `classify_zstar` returns the posterior mode of the jointly most likely state. 

```{r}
print(
  str(classify_alpha(myFit))
)

print(
  str(classify_gamma(myFit))
)

print(
  str(classify_zstar(myFit))
)
```

## 4. Diagnose

This step is not implemented as of the date of this writing. We plan to explore how to diagnose a HMM outside posterior predictive checks (think, for example, pseudo residuals).

## 5. Visualize

We expect that visualization will play a very important role in our software once it arrives to a mature stage. At this point, we support a small amount of graphical routines:

* `plot_series` focuses on the observation variables.
* `plot_state_probability` focuses on the estimated assignment probability.
* `plot_ppredictive` focuses on posterior predictive checks.

Besides functions for fully-customizable individual plots such as ` plot_ppredictive_density(y, yPred)` (note that the function takes as arguments the actual values and not a fitted object), the software includes a convenient way to automatically include one or more plots from the same family on a single layout. Each plot may toggle one or more "feature" such as colored lines, points, or backgrounds. Additionally, plots adjust automatically for multivariate observations, and variable names are automatically shown if the data matrix used to fitted the model has named columns. Colors are themeable via global options.

The following features are currently available:

* When plotting the observation variables:
		* `stateShade` to shade the background based on the state probability.
		* `yColoredLine` to include a line colored according to the state probability.
		* `yColoredDots` to include dots colored according to the state probability.

* When plotting the state probabilities:
		* `stateShade` to shade the background based on the state probability.
		* `probabilityColoredLine` to include a line colored according to the state probability.
		* `probabilityColoredDots` to include dots colored according to the state probability.
		* `bottomColoredMarks` and `topColoredMarks` to include colored tick marks on the bottom or top axis.
		* `probabilityFan` to show the posterior interval of assignment probabilities.

Again, the objects returned by `fit` and `sampling` are compatible with the rstan ecosystem, including visualization routines in the bayesplot package.

```{r, results = "hide"}
mySpec <- hmm(
  K = 3, R = 2,
  observation =
    Gaussian(mu = Gaussian(mu = -10, sigma = 1), sigma = 1) +
    Gaussian(mu = Gaussian(mu =   0, sigma = 1), sigma = 1) +
    Gaussian(mu = Gaussian(mu =  10, sigma = 1), sigma = 1),
  initial     = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    Dirichlet(alpha = c(1.0, 0.2, 0.2)) +
    Dirichlet(alpha = c(0.2, 1.0, 0.2)) +
    Dirichlet(alpha = c(0.2, 0.2, 1.0)),
  name = "Univariate Gaussian Dummy Model"
)

mySim <- sim(mySpec, T = 300, seed = 9000, iter = 500, chains = 1)
ySim  <- extract_ypred(mySim)[[1]][1, , ]
colnames(ySim) <- c("Weight", "Height")

myFit <- fit(mySpec, y = ySim, seed = 9000, iter = 500, chains = 1)
```

```{r}
plot_series(myFit, legend.cex = 0.8)

plot_series(myFit, xlab = "Time steps", features = c("yColoredLine"))

plot_series(
  myFit, stateProbability =  "smoothed", 
  features = c("stateShade", "bottomColoredMarks")
)

plot_state_probability(myFit, main = "Title", xlab = "Time")

plot_state_probability(myFit, features = c("stateShade"), main = "Title", xlab = "Time")

# plot_state_probability(myFit, features = c("bottomColoredMarks"), main = "Title", xlab = "Time")

plot_state_probability(myFit, features = c("probabilityColoredDots"), main = "Title", xlab = "Time")

plot_state_probability(myFit, features = c("probabilityColoredLine"), main = "Title", xlab = "Time")

# plot_state_probability(myFit, stateProbability = "filtered", features = c("bottomColoredMarks", "probabilityFan"), stateProbabilityInterval = c(0.05, 0.95), main = "Title", xlab = "Time")

plot_ppredictive(myFit, type = c("density", "cumulative", "summary"), fun = median)

plot_ppredictive(myFit, type = c("density", "boxplot"), fun = median, subset = 1:10)

plot_ppredictive(
  myFit, 
  type = c("density", "boxplot", "scatter"), 
  fun = median, fun1 = mean, fun2 = median, 
  subset = 1:40
)

plot_ppredictive(myFit, type = c("density", "cumulative", "hist"), fun = median)

plot_ppredictive(myFit, type = c("density", "cumulative", "ks"))
```

## 6. Select

Model comparison is an important step in the statistical workflow, and future versions of the software will provide information criterion and other quantities for model selection.

# Mixture models

Mixture models may be seen as a simplified version of HMM, where the assignment probabilities do not have a Markovian structure. We provide a simple implementation, and future iterations of our software will include extracting the individual assignment probabilities, and support for visualization and other data analysis procedures specific to mixture models.

```{r, results = "hide"}
mySpec <- mixture(
  K = 3, R = 1,
  observation = Student(
    mu    = Default(),
    sigma = Gaussian(0,  10, bounds = list(0, NULL)),
    nu    = Gaussian(0, 100, bounds = list(0, NULL))
  ),
  initial     = Dirichlet(alpha = Default()),
  name = "Univariate Student t Mixture"
)

set.seed(9000)
y <- as.matrix(
  c(rnorm(50, 5, 1), rnorm(300, 0, 1), rnorm(100, -5, 1))
)

myFit <- fit(mySpec, y = y, chains = 1, iter = 500)
```

```{r}
plot_obs(myFit)

print_all(myFit)
```

# Appendix
\label{sec:appendix}

We present 19 different possible configurations.

```{r, warning = FALSE}
# OBSERVATION MODEL -------------------------------------------------------

K = 3
R = 2

# Case 1. A different multivariate density for each state
#   Input: K multivariate densities
#   Behaviour: Nothing

exCase1  <- hmm(
  K = K, R = R,
  observation =
    MVGaussianCor(
      mu    = Gaussian(mu = 0, sigma = 1),
      L     = LKJCor(eta = 2)
    ) +
    MVGaussianCor(
      mu    = Gaussian(mu = 0, sigma = 10),
      L     = LKJCor(eta = 3)
    ) +
    MVGaussianCor(
      mu    = Gaussian(mu = 0, sigma = 100),
      L     = LKJCor(eta = 4)
    ),
  initial     = Default(),
  transition  = Default(),
  name = "A different multivariate density for each each state"
)

# Case 2. Same multivariate density for every state
#   Input: One multivariate density
#   Behaviour: Repeat input K times

exCase2  <- hmm(
  K = K, R = R,
  observation =
    MVGaussianCor(
      mu    = Gaussian(mu = 0, sigma = 100),
      L     = LKJCor(eta = 2)
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Same multivariate density for every state"
)

# Case 3. Same univariate density for every state and every output variable
#   Input: One univariate density
#   Behaviour: Repeat input K %nested% R times

exCase3  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Same univariate density for every state and every output variable"
)

# Case 4. Same R univariate densities for every state
#   Input: R univariate densities
#   Behaviour: Repeat input K times

exCase4  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Same R univariate densities for every state"
)

# Case 5. Same univariate density for every output variable
#   Input: K univariate densities
#   Behaviour: Repeat input R times

exCase5  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Same univariate density for every output variable"
)

# Case 6. Different univariate densities for every pair of state and output variable
#   Input: K %nested% R univariate densities
#   Behaviour: None

exCase6  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Different univariate densities for every pair of state and output variable"
)

# UNIVARIATE Observation model --------------------------------------------

K = 3
R = 1

# Case 7. A different univariate density for each each state
#   Input: K univariate densities
#   Behaviour: Nothing

exCase7  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "A different univariate density for each each state"
)

# Case 8. Same univariate density for every state
#   Input: One univariate density
#   Behaviour: Repeat input K times

exCase8  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Same multivariate density for every state"
)

# INITIAL MODEL -----------------------------------------------------------
K = 3
R = 2

# Case 9. Same univariate density for every initial state
#   Input: One univariate density
#   Behaviour: Repeat input K times
exCase9  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial =
    Beta(
      alpha = Gaussian(0, 1),
      beta  = Gaussian(1, 10)
    ),
  transition  = Default(),
  name = "Same univariate density for every initial state"
)

# Case 10. One multivariate density for the whole initial vector
#   Input: One multivariate density
#   Behaviour: Nothing
exCase10  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial =
    Dirichlet(
      alpha = Default()
    ),
  transition  = Default(),
  name = "One multivariate density for the whole initial vector"
)

# Case 11. A different univariate density for each initial state
#   Input: K univariate densities
#   Behaviour: Nothing
exCase11 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  transition  = Default(),
  name = "A different univariate density for each initial state"
)

# Case 12. A link
#   Input: One link density
#   Behaviour: Nothing
exCase12 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial =
    InitialSoftmax(
      vBeta = Default()
    ),
  transition  = Default(),
  name = "TV Initial distribution"
)

# TRANSITION MODEL --------------------------------------------------------
K = 3
R = 2

# Case 13. Same univariate density for every transition
#   Input: One univariate density
#   Behaviour: Repeat input KxK times
exCase13 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    Gaussian(mu = 0, sigma = 1),
  name = "Same univariate density for every transition"
)

# Case 14. Same multivariate density for every transition row
#   Input: One multivariate density
#   Behaviour: Repeat input K times

exCase14 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    Dirichlet(
      alpha = c(0.5, 0.5, 0.7)
    ),
  name = "Same multivariate density for every transition row"
)

# Case 15. A different univariate density for each element of the transition row
#   Input: K univariate densities
#   Behaviour: Repeat input K times

exCase15 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    Beta(alpha = 0.1, beta = 0.1) +
    Beta(alpha = 0.5, beta = 0.5) +
    Beta(alpha = 0.9, beta = 0.9),
  name = "A different univariate density for each element of the transition row"
)

# Case 16. A different multivariate density for each transition row
#   Input: K multivariate densities
#   Behaviour: nothing

exCase16 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    Dirichlet(alpha = c(0.1, 0.1, 0.1)) +
    Dirichlet(alpha = c(0.5, 0.5, 0.5)) +
    Dirichlet(alpha = c(0.9, 0.9, 0.9)),
  name = "A different multivariate density for each transition row"
)

# Case 17. Different univariate densities for each element of the transition matrix
#   Input: KxK univariate densities
#   Behaviour: None

exCase17 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    Beta(alpha = 0.1, beta = 0.1) +
    Beta(alpha = 0.2, beta = 0.2) +
    Beta(alpha = 0.3, beta = 0.3) +
    Beta(alpha = 0.4, beta = 0.4) +
    Beta(alpha = 0.5, beta = 0.5) +
    Beta(alpha = 0.6, beta = 0.6) +
    Beta(alpha = 0.7, beta = 0.7) +
    Beta(alpha = 0.8, beta = 0.8) +
    Beta(alpha = 0.9, beta = 0.9),
  name = "Different univariate densities for each element of the transition matrix"
)

# Case 18. A link
#   Input: One link density
#   Behaviour: Nothing

exCase18 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    TransitionSoftmax(
      uBeta = Gaussian(mu = 0, sigma = 1)
    ),
  name = "Different univariate densities for each element of the transition matrix"
)

# A FULLY COMPLEX MODEL ---------------------------------------------------

# Case 19. A link in both the transition and initial distribution.
#   Input: A link in both the transition and initial distribution.
#   Behaviour: Nothing
exCase19 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial =
    InitialSoftmax(
      vBeta = Gaussian(mu = 0, sigma = 1)
    ),
  transition  =
    TransitionSoftmax(
      uBeta = Gaussian(mu = 0, sigma = 1)
    ),
  name = "Fully Complex Model"
)
```
