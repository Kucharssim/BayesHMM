---
title: "An introduction to BayesHMM"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{An introduction to BayesHMM}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo     = TRUE, 
  eval     = TRUE, 
  cache    = TRUE,
  collapse = TRUE,
  comment  = "#>"
)
```

% Math operators
\newcommand{\argmax}{\arg\!\max}
\newcommand{\argmin}{\arg\!\min}
\newcommand\ev[1]{E\left\langle#1\right\rangle}
\newcommand\vv[1]{V\left\langle#1\right\rangle}

% Math commands
\newcommand{\mat}[1]{\mathbf{#1}}

% Math symbols
\renewcommand{\DD}{\mathcal{D}}
\renewcommand{\NN}{\mathcal{N}}
\renewcommand{\UU}{\mathcal{U}}
\renewcommand{\LL}{\mathcal{L}}
\renewcommand{\RR}{\mathbb{R}}

BayesHMM is an R Package to run full Bayesian inference on Hidden Markov Models (HMM) using the probabilistic programming language Stan. By providing an intuitive, expressive yet flexible input interface, we enable non-technical users to carry out research using the Bayesian workflow. We provide the user with an expressive interface to mix and match a wide array of options for the observation and latent models, including ample choices of densities, priors, and link functions whenever covariates are present. The software enables users to fit HMM with time-homogeneous transitions as well as time-varying transition probabilities. Priors can be set for every model parameter. Implemented inference algorithms include forward (filtering), forward-backwards (smoothing), Viterbi (most likely hidden path), prior predictive sampling, and posterior predictive sampling. Graphs, tables and other convenience methods for convergence diagnosis, goodness of fit, and data analysis are provided.

This vignette introduces the software and briefly reviews current capabilities. Although we walk the reader through most of the functionalities, we do not discuss function parameters and other details typically reviewed in the documentations. A list of future features may be found in the [Roadmap wiki page](https://github.com/luisdamiano/BayesHMM/wiki/Roadmap). We acknowledge [Google Summer of Code 2018](https://summerofcode.withgoogle.com/projects/#4681157036212224) for funding.

__NOTE__: Our software is work in progress. We will review naming conventions and other design decisions at a later stage.

# Introduction

# Hidden Markov Models

Hidden Markov Models (HMM) involve two interconnected models. The state model consists of a discrete-time, discrete-state first-order Markov chain $z_t \in \{1, \dots, K\}$ that transitions according to $p(z_t | z_{t-1})$. In turns, the observation model is governed by $p(\mat{y}_t | z_t)$, where $\mat{y}_t \in \RR^R$ are the observations, emissions or output. The corresponding joint distribution is

\[
p(\mat{z}_{1:T}, \mat{y}_{1:T})
  = p(\mat{z}_{1:T}) p(\mat{y}_{1:T} | \mat{z}_{1:T})
  = \left[ p(z_1) \prod_{t=2}^{T}{p(z_t | z_{t-1})} \right] \left[ \prod_{t=1}^{T}{p(\mat{y}_t | z_{t})} \right].
\]

The non-stochastic quantities of the model are the length of the observed sequence $T$ and the number of hidden states $K$. The observed sequence $\mat{y}_t$ is a stochastic known quantity. The parameters of the models are $\mat{\theta} = (\mat{\pi}, \mat{\theta}_t, \mat{\theta}_o)$, where $\mat{\pi}$ are the probabilities of the initial state distribution, $\mat{\theta}_t$ are the parameters of the transition model and $\mat{\theta}_o$ are the parameters of the state-conditional density function $p(\mat{y}_t | z_t)$. Their forms depend on the characteristics of the model specified by the user.

## Observation model

The observation model is specified by the density $p(\mat{y}_t | z_{t})$. When the output is discrete, it commonly takes the form of an observation matrix

\[
p(\mat{y}_t | z_t = k, \mat{\theta}) = \text{Categorical}(\mat{y}_t | \mat{\theta}_k).
\]

If the output is continuous, observations may follow for example a conditional Gaussian distribution

\[
p(\mat{y}_t | z_t = k, \mat{\theta}) = \mathcal{N}(\mat{y}_t | \mat{\mu}_k, \mat{\Sigma}_k).
\]

Alternatively, time-varying covariates $\mat{x}_t \in \RR^M$ may be used to drive the location parameter of the chose density,

\[
p(\mat{y}_t | \mat{x}_t, z_t = k, \mat{\theta}) = \mathcal{N}(\mat{y}_t | \mat{x}_t \mat{\beta}^x_k, \mat{\Sigma}_k).
\]

## Transition model

In the most common case of time-homogeneous HMMs, state transitions are characterized by the $K \times K$ sized transition matrix with simplex rows $\mat{A} = \{a_{ij}\}$ with $a_{ij} = p(z_t = j | z_{t-1} = i)$. There are $K \times (K-1)$ free parameters as the rows of the matrix must sum to one.

Alternatively, time-varying covariates $\mat{u}_t \in \RR^P$ may be used to drive the location parameter of the chosen density, effectively allowing for time-varying transition probabilities. The model involves a multinomial regression whose parameters depend on the previous state taking the value $i$. Using the softmax transform, for example,

\[
p(z_t | \mat{u}_{t}, z_{t-1} = i) = \text{softmax}(\mat{u}_{t} \mat{\beta}^u_i).
\]

## Initial distribution model

Most frequently in applications, the initial distribution model is characterized by a $K$ sized simplex $\mat{\pi}$ with initial probabilities $\pi_{i} = p(z_1 = i)$. Alternatively, the first observation may be assigned to a state via a multinomial regression if relevant information is avaliable in the form of a covariate vector $\mat{v} \in \RR^Q$,

\[
p(z_1 | \mat{v}, \mat{\theta}) = \text{softmax}(\mat{v} \mat{\beta}^v).
\]

## Naming convention

Documentation and software adapt the following naming convention. Minor modifications may be found due to syntax restrictions in R or Stan.

```
Constants
R		    Observation dimension
K		    Number of hidden states
M       Number of covariates for the observation model
P       Number of covariates for the transition model
Q       Number of covariates for the initial model

Covariates
x_t		  Time-varying covariates for the observation model
u_t		  Time-varying covariates for the transition model
v  		  Covariates for the initial model

Known-stochastic quantities
y_t		  Observation vector

Model parameters
*_kr		Ex. mu_11, sigma_11 (k, r suffixes are optional)
A		    Transition model parameters (if no covariates)
pi		  Initial distribution parameters (if no covariates)
xBeta	  Regression parameters for the observation model
uBeta	  Regression parameters for the transition model
vBeta	  Regression parameters for the initial model

Estimated quantities
z_t 	  Hidden state
alpha_t	Filtered probability
gamma_t	Smoothed probability
zstar		Viterbi

(Prior/Posterior) predictive quantities
yPred		Sample of observations drawn from the  predictive density
zPred		Sample of latent path drawn from the  predictive density
```

# Using BayesHMM

The typical workflow is (1) specify, (2) validate, (3) fit, (4) diagnose, (5) visualize, (6) compare models. Only steps 1:5 are currently implemented as of now.

You may install the last version of our software from GitHub. Please note that BayesHMM requires rstan (>= 2.17), the R interface for the probabilistic programming language [Stan](http://mc-stan.org/).

```{r include = FALSE}
library(BayesHMM)
```

```{r eval = FALSE}
devtools::install_github("luisdamiano/BayesHMM", ref = "master")
library(BayesHMM)
```

## 1. Specify

A HMM may be specified by calling the `hmm` function:

```{r, eval = FALSE}
hmm(
  K = 3, R = 2,
  observation = {...}
  initial     = {...}
  transition  = {...}
  name = "Model name"
)
```

where $K$ is the number of hidden states and $R$ is the number of dimensions in the observation variable.

Observation, initial, and transition rely on ` Density` S3 objects to specify density form, parameter priors, and fixed values for parameters. For example, `Gaussian(mu = 0, sigma = 1)` specifies a Gaussian density with fixed parameters, while `Gaussian(mu = Gaussian(0, 10), sigma = Cauchy(0, 10, bounds = list(0, NA)))` specifies a Gaussian density with a Gaussian prior on the location parameter and a Cauchy prior on the scale parameter, which is bounded on $[0, \infty)$. User may specify bounds in the parameter space as well as truncation in prior densities.

The followings are currently avaliable:

TABLE ABOUT HERE

### Observation model

The observation model may be specified in any of the following ways:

  |R    | Length  | Density             | FUN         |
  |-----|---------|---------------------|-------------|
  | 1   | 1       | UnivariateDensity   | repeat_K    |
  | 1   | K       | UnivariateDensity   | repeat_none |
  |>1   | 1       | MultivariateDensity | repeat_K    |
  |>1   | 1       | UnivariateDensity   | repeat_KxR  |
  |>1   | K       | MultivariateDensity | repeat_none |
  |>1   | K       | UnivariateDensity   | repeat_R    |
  |>1   | R       | UnivariateDensity   | repeat_K    |
  |>1   | KxR     | UnivariateDensity   | repeat_none |

For a univariate model (i.e. $R = 1$):

  * User enters one univariate density: specify the same density in each hidden state variable.
  * User enters $K$ univariate density: specify one different density in each hidden state (ex. Gaussian in one state and Student in the other).
  
This system is flexible enough to let the user specify different densities and priors for each state. For example, the following snippet specify a (rather unrealistic) model where each state are a priori believed to have negative, near zero, and positive location parameters.
      
```{r}
mySpec <- hmm(
  K = 3, R = 1,
  observation =
    Gaussian(mu = -10, sigma = 10) +
    Gaussian(mu =   0, sigma = 10) +
    Gaussian(mu =  10, sigma = 10),
  initial     = Dirichlet(alpha = Default()),
  transition  = Dirichlet(alpha = Default()),
  name = "Univariate Gaussian"
)
```

For a multivariate model ($R > 1$):

  * User enters one univariate density: same density for each dimension of the observation variable in all states (i.e. same density and priors for the R variables in the K latent states).
  * User enters one multivariate density: same density for the observation vector in all hidden states.
  * User enters $K$ univariate density: each dimension of the observation variable has the same specification across states.
  * User enters $K$ multivariate density: specify a multivariate density for the observation vector which changes per state.
  * User enters $R$ univariate density: specify one density for each element of the observation vector, which is the same across state.
  * User enters $R \times K$ univariate density: specify density for each element of the observation vector in each state. Order is $k_1r_1, \dots, k_1, r_R, k_2r_1, \dots, k_2r_R, \dots, k_Kr_1, \dots, k_K,r_R$.

### Transition model

The user may enter a `Density` S3 object such as a `Dirichlet`, or a `LinkDensity` S3 object (inherits from `Density`) such as `TransitionSoftmax()`. The latter allows to specify priors for the transition regression parameter vector. For example, the following snippet sets regularizing priors on the regression coefficients. Note that this model is hard to identify, although we achieved satisfactory results optimizing the posterior density (i.e. maximum a posterior estimates).

```{r}
mySpec <- hmm(
  K = 2, R = 1,
  observation = Gaussian(
    mu    = Gaussian(0, 10),
    sigma = Student(mu = 0, sigma = 10, nu = 1, bounds = list(0, NULL))
  ),
  initial     = Default(),
  transition  = TransitionSoftmax(
    uBeta = Gaussian(0, 5)
  ),
  name = "TVHMM Softmax Univariate Gaussian"
)
```

Internally, a specification stores either $K$ multivariate densities (i.e. one multivariate density for each row in the transition matrix) or $K\timesK$ univariate densities (i.e. one univariate density for each element in the transition matrix). Densities are recycled accordingly.

  | Length  | Density             | FUN                       |
  |---------|---------------------|-------------------------- |
  | 1       | UnivariateDensity   | repeat_KxK                |
  | 1       | MultivariateDensity | repeat_K                  |
  | 1       | LinkDensity         | nothing                   |
  | K       | UnivariateDensity   | repeat_K                  |
  | K       | MultivariateDensity | repeat_none_multivariate  |
  | KxK     | UnivariateDensity   | repeat_none_univariate    |

__NOTE__: Unfortunately, we haven't implemented fixed values for transition parameters see.

### Initial model

Internally, a specification stores either one multivariate densities for the whole initial probability vector, or $K$ univariate densities for each element of said vector. Densities are recycled accordingly.

  | Length  | Density             | FUN
  | --------|---------------------|--------------------------
  | 1       | UnivariateDensity   | repeat_K
  | 1       | MultivariateDensity | repeat_none_multivariate
  | 1       | LinkDensity         | repeat_none_multivariate
  | K       | UnivariateDensity   | repeat_none_univariate

```{r}
mySpec <- hmm(
  K = 2, R = 1,
  observation = Gaussian(
    mu    = Gaussian(0, 10),
    sigma = Student(mu = 0, sigma = 10, nu = 1, bounds = list(0, NULL))
  ),
  initial     = InitialSoftmax(
    vBeta = Gaussian(0, 5)
  ),
  transition  = Dirichlet(alpha = Default()),
  name = "TVHMM Softmax Univariate Gaussian"
)
```

### Examples

We present 19 different possible configurations.

```{r, warning = FALSE}
# OBSERVATION MODEL -------------------------------------------------------

K = 3
R = 2

# Case 1. A different multivariate density for each state
#   Input: K multivariate densities
#   Behaviour: Nothing

exCase1  <- hmm(
  K = K, R = R,
  observation =
    MVGaussianCor(
      mu    = Gaussian(mu = 0, sigma = 1),
      L     = LKJCor(eta = 2)
    ) +
    MVGaussianCor(
      mu    = Gaussian(mu = 0, sigma = 10),
      L     = LKJCor(eta = 3)
    ) +
    MVGaussianCor(
      mu    = Gaussian(mu = 0, sigma = 100),
      L     = LKJCor(eta = 4)
    ),
  initial     = Default(),
  transition  = Default(),
  name = "A different multivariate density for each each state"
)

write_model(exCase1, noLogLike = FALSE, "out")

# Case 2. Same multivariate density for every state
#   Input: One multivariate density
#   Behaviour: Repeat input K times

exCase2  <- hmm(
  K = K, R = R,
  observation =
    MVGaussianCor(
      mu    = Gaussian(mu = 0, sigma = 100),
      L     = LKJCor(eta = 2)
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Same multivariate density for every state"
)

write_model(exCase2, noLogLike = FALSE, "out")

# Case 3. Same univariate density for every state and every output variable
#   Input: One univariate density
#   Behaviour: Repeat input K %nested% R times

exCase3  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Same univariate density for every state and every output variable"
)

write_model(exCase3, noLogLike = FALSE, "out")

# Case 4. Same R univariate densities for every state
#   Input: R univariate densities
#   Behaviour: Repeat input K times

exCase4  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Same R univariate densities for every state"
)

write_model(exCase4, noLogLike = FALSE, "out")

# Case 5. Same univariate density for every output variable
#   Input: K univariate densities
#   Behaviour: Repeat input R times

exCase5  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Same univariate density for every output variable"
)

write_model(exCase5, noLogLike = FALSE, "out")

# Case 6. Different univariate densities for every pair of state and output variable
#   Input: K %nested% R univariate densities
#   Behaviour: None

exCase6  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Different univariate densities for every pair of state and output variable"
)

write_model(exCase6, noLogLike = FALSE, "out")

# UNIVARIATE Observation model --------------------------------------------

K = 3
R = 1

# Case 7. A different univariate density for each each state
#   Input: K univariate densities
#   Behaviour: Nothing

exCase7  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "A different univariate density for each each state"
)

write_model(exCase7, noLogLike = FALSE, "out")

# Case 8. Same univariate density for every state
#   Input: One univariate density
#   Behaviour: Repeat input K times

exCase8  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial     = Default(),
  transition  = Default(),
  name = "Same multivariate density for every state"
)

write_model(exCase8, noLogLike = FALSE, "out")

# INITIAL MODEL -----------------------------------------------------------
K = 3
R = 2

# Case 9. Same univariate density for every initial state
#   Input: One univariate density
#   Behaviour: Repeat input K times
exCase9  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial =
    Beta(
      alpha = Gaussian(0, 1),
      beta  = Gaussian(1, 10)
    ),
  transition  = Default(),
  name = "Same univariate density for every initial state"
)

write_model(exCase9, noLogLike = FALSE, "out")

# Case 10. One multivariate density for the whole initial vector
#   Input: One multivariate density
#   Behaviour: Nothing
exCase10  <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial =
    Dirichlet(
      alpha = Default()
    ),
  transition  = Default(),
  name = "One multivariate density for the whole initial vector"
)

write_model(exCase10, noLogLike = FALSE, "out")

# Case 11. A different univariate density for each initial state
#   Input: K univariate densities
#   Behaviour: Nothing
exCase11 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  transition  = Default(),
  name = "A different univariate density for each initial state"
)

write_model(exCase11, noLogLike = FALSE, "out")

# Case 12. A link
#   Input: One link density
#   Behaviour: Nothing
exCase12 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial =
    InitialSoftmax(
      vBeta = Default()
    ),
  transition  = Default(),
  name = "TV Initial distribution"
)

write_model(exCase12, noLogLike = FALSE, "out")

# TRANSITION MODEL --------------------------------------------------------
K = 3
R = 2

# Case 13. Same univariate density for every transition
#   Input: One univariate density
#   Behaviour: Repeat input KxK times
exCase13 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    Gaussian(mu = 0, sigma = 1),
  name = "Same univariate density for every transition"
)

write_model(exCase13, noLogLike = FALSE, "out")

# Case 14. Same multivariate density for every transition row
#   Input: One multivariate density
#   Behaviour: Repeat input K times

exCase14 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    Dirichlet(
      alpha = c(0.5, 0.5, 0.7)
    ),
  name = "Same multivariate density for every transition row"
)

write_model(exCase14, noLogLike = FALSE, "out")

# Case 15. A different univariate density for each element of the transition row
#   Input: K univariate densities
#   Behaviour: Repeat input K times

exCase15 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    Beta(alpha = 0.1, beta = 0.1) +
    Beta(alpha = 0.5, beta = 0.5) +
    Beta(alpha = 0.9, beta = 0.9),
  name = "A different univariate density for each element of the transition row"
)

write_model(exCase15, noLogLike = FALSE, "out")

# Case 16. A different multivariate density for each transition row
#   Input: K multivariate densities
#   Behaviour: nothing

exCase16 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    Dirichlet(alpha = c(0.1, 0.1, 0.1)) +
    Dirichlet(alpha = c(0.5, 0.5, 0.5)) +
    Dirichlet(alpha = c(0.9, 0.9, 0.9)),
  name = "A different multivariate density for each transition row"
)

write_model(exCase16, noLogLike = FALSE, "out")

# Case 17. Different univariate densities for each element of the transition matrix
#   Input: KxK univariate densities
#   Behaviour: None

exCase17 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    Beta(alpha = 0.1, beta = 0.1) +
    Beta(alpha = 0.2, beta = 0.2) +
    Beta(alpha = 0.3, beta = 0.3) +
    Beta(alpha = 0.4, beta = 0.4) +
    Beta(alpha = 0.5, beta = 0.5) +
    Beta(alpha = 0.6, beta = 0.6) +
    Beta(alpha = 0.7, beta = 0.7) +
    Beta(alpha = 0.8, beta = 0.8) +
    Beta(alpha = 0.9, beta = 0.9),
  name = "Different univariate densities for each element of the transition matrix"
)

write_model(exCase17, noLogLike = FALSE, "out")

# Case 18. A link
#   Input: One link density
#   Behaviour: Nothing

exCase18 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
  transition  =
    TransitionSoftmax(
      uBeta = Gaussian(mu = 0, sigma = 1)
    ),
  name = "Different univariate densities for each element of the transition matrix"
)

write_model(exCase18, noLogLike = FALSE, "out")

# A FULLY COMPLEX MODEL ---------------------------------------------------

# Case 19. A link in both the transition and initial distribution.
#   Input: A link in both the transition and initial distribution.
#   Behaviour: Nothing
exCase19 <- hmm(
  K = K, R = R,
  observation =
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ) +
    Gaussian(
      mu    = Gaussian(0, 10),
      sigma = Gaussian(0, 10, bounds = list(0, NULL))
    ),
  initial =
    InitialSoftmax(
      vBeta = Gaussian(mu = 0, sigma = 1)
    ),
  transition  =
    TransitionSoftmax(
      uBeta = Gaussian(mu = 0, sigma = 1)
    ),
  name = "Fully Complex Model"
)

write_model(exCase19, noLogLike = FALSE, "out")
```

## 2. Validate

Bayesian Software Validation allows users to test software accuracy. We provide a one-liner to run a validation protocol based on the prior predictive density. Iterations are run in parallel using `foreach` and `doParallel`.

__Validation protocol inspired by Simulation Based Calibration (cite)__

* Compile the prior predictive model (i.e. no likelihood statement in the Stan code).
* Draw $N$ samples of the parameter vector $\theta$ and the observation vector $y$ from prior predictive density.
* Compile the posterior predictive model (i.e. Stan code includes both prior density and likelihood statement).
* For all $n \in 1, \dots, N$ ...
    * Feed $y^{(n)}$ to the full model.
    * Draw posterior samples for the observation variable $y^{(n)}_{\text{new}}$.
    * Collect Hamiltoniam Monte Carlo diagnostics: number of divergences, number of times it hits max tree depth, maximum leapfrogs, warmup and sample times.
    * Collect posterior sampling diagnostics: posterior summary measures (mean, sd, quantiles), comparison against the true value (rank), MCMC convergence measures (Monte Carlo SE, ESS, RHat).
    * Collect posterior predictive diagnostics: observation ranks, Kolmogorov-Smirnov statistic for observed sample vs posterior predictive samples.
    * Other: we expect to implement a better protocol in future iterations.

There is no such a thing as an universal, automatic way to validate models. The software computes and summarizes different diagnostic measures and provides the user with high-level tools to assess calibration (including printouts, tables, and plots). Because the user may want to single out and inspect one of the iterations, in future versions it will be able to recover (or reproduce) the full posterior sample drawn in the $n$-th iteration.

In the following snippet, we specify a HMM and validate its calibration.

```{r}
  mySpec <- hmm(
    K = 3, R = 2,
    observation =
      Gaussian(mu = Gaussian(mu = -10, sigma = 1), sigma = 1) +
      Gaussian(mu = Gaussian(mu =   0, sigma = 1), sigma = 1) +
      Gaussian(mu = Gaussian(mu =  10, sigma = 1), sigma = 1),
    initial     = Dirichlet(alpha = c(0.5, 0.5, 0.5)),
    transition  =
      Dirichlet(alpha = c(1.0, 0.2, 0.2)) +
      Dirichlet(alpha = c(0.2, 1.0, 0.2)) +
      Dirichlet(alpha = c(0.2, 0.2, 1.0)),
    name = "Univariate Gaussian Dummy Model"
  )

  val <- validate_calibration(mySpec, N = 2, T = 100, iter = 500, seed = 9000)
```

The function returns a named list with two elements, `chains` and `parameters`, which are data frames with the corresponding summary measures.

```{r}
knitr::kable(head(val$chains), digits = 2, caption = "Chains diagnostics")
knitr::kable(head(val$parameters), digits = 2, caption = "Parameters diagnostics")
plot(val$parameters$Rhat, type = "h", ylim = c(0.9, 1.15))
abline(h = 1.1)
# Sort to fix label switching
```

More than one density may be chained using the plus sign operator `+`, which creates a `DensityList` S3 object under the hood. <!-- Internally, the package includes some convenience functions such as `densityApply()`, which applies a function to any (possibly nested) density in the list. --> 
